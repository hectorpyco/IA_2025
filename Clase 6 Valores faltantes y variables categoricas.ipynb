{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO+HK8vgtSUs4ZQ63sgE1gO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## üß≠ Introducci√≥n\n","\n","En esta pr√°ctica trabajaremos con el dataset real **Titanic**, muy utilizado en ciencia de datos por su riqueza y variedad de variables. A diferencia de los datasets anteriores, este contiene:\n","\n","- **Campos vac√≠os (valores faltantes)** en varias columnas.\n","- **Variables categ√≥ricas**, como nombres, g√©nero, clases del pasaje, entre otras.\n","- Mezcla de **datos num√©ricos y no num√©ricos**, lo que requiere un preprocesamiento m√°s cuidadoso.\n","\n","El objetivo de esta actividad es practicar conceptos fundamentales del tratamiento de datos reales antes del entrenamiento de modelos:\n","\n","- üîç Identificar y tratar **valores faltantes**\n","- üî° Codificar **variables categ√≥ricas** (label encoding, one-hot encoding)\n","- üßπ Preparar un conjunto de datos limpio, num√©rico y listo para aplicar modelos de machine learning\n","\n","Este paso es clave en cualquier proyecto de ciencia de datos y nos permite adaptar los datos a las exigencias de los algoritmos de aprendizaje autom√°tico.\n","\n","---\n"],"metadata":{"id":"GOHEr_P_cSip"}},{"cell_type":"markdown","source":["# üîπ Paso 1: Carga del dataset Titanic\n","üìå El dataset \"Titanic\" contiene informaci√≥n sobre los pasajeros del famoso barco. Es √∫til para tareas de clasificaci√≥n: por ejemplo, predecir qui√©n sobrevivi√≥ (survived)."],"metadata":{"id":"8ie-pH57eqpA"}},{"cell_type":"code","source":["# üö¢ Cargar el dataset Titanic desde seaborn y explorar su contenido\n","import seaborn as sns\n","import pandas as pd\n","\n","# Cargar el dataset como un DataFrame\n","df = sns.load_dataset(\"titanic\")\n","\n","# Mostrar las primeras 5 filas\n","print(\"Primeras filas: \",  \"\\n\", df.head(),  \"\\n\") # El '\\n' representa un salto de l√≠nea\n","\n","# Mostrar la forma del dataset (filas, columnas)\n","print(\"Filas y columnas: \", df.shape,  \"\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1wsWlUncX65","executionInfo":{"status":"ok","timestamp":1744639289649,"user_tz":180,"elapsed":54,"user":{"displayName":"Hector Estigarribia FCyT UNCA","userId":"04796158545147043803"}},"outputId":"fe250eea-64c6-4e88-f831-7ba3a4de59ad"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras filas:  \n","    survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n","0         0       3    male  22.0      1      0   7.2500        S  Third   \n","1         1       1  female  38.0      1      0  71.2833        C  First   \n","2         1       3  female  26.0      0      0   7.9250        S  Third   \n","3         1       1  female  35.0      1      0  53.1000        S  First   \n","4         0       3    male  35.0      0      0   8.0500        S  Third   \n","\n","     who  adult_male deck  embark_town alive  alone  \n","0    man        True  NaN  Southampton    no  False  \n","1  woman       False    C    Cherbourg   yes  False  \n","2  woman       False  NaN  Southampton   yes   True  \n","3  woman       False    C  Southampton   yes  False  \n","4    man        True  NaN  Southampton    no   True   \n","\n","Filas y columnas:  (891, 15) \n","\n"]}]},{"cell_type":"code","source":["# Informaci√≥n general de columnas, tipos y valores faltantes\n","print(\"Informaci√≥n general:\", \"\\n\")\n","print( df.info(),  \"\\n\")\n","\n","# Estad√≠sticas descriptivas de columnas num√©ricas\n","print(\"Estad√≠sticas descriptivas de columnas num√©ricas: \", \"\\n\", df.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgvivuUXmIJE","executionInfo":{"status":"ok","timestamp":1744639263426,"user_tz":180,"elapsed":17,"user":{"displayName":"Hector Estigarribia FCyT UNCA","userId":"04796158545147043803"}},"outputId":"5bc94b8e-3a6c-4918-d5f7-8f5e9244b8a0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Informaci√≥n general: \n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 15 columns):\n"," #   Column       Non-Null Count  Dtype   \n","---  ------       --------------  -----   \n"," 0   survived     891 non-null    int64   \n"," 1   pclass       891 non-null    int64   \n"," 2   sex          891 non-null    object  \n"," 3   age          714 non-null    float64 \n"," 4   sibsp        891 non-null    int64   \n"," 5   parch        891 non-null    int64   \n"," 6   fare         891 non-null    float64 \n"," 7   embarked     889 non-null    object  \n"," 8   class        891 non-null    category\n"," 9   who          891 non-null    object  \n"," 10  adult_male   891 non-null    bool    \n"," 11  deck         203 non-null    category\n"," 12  embark_town  889 non-null    object  \n"," 13  alive        891 non-null    object  \n"," 14  alone        891 non-null    bool    \n","dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n","memory usage: 80.7+ KB\n","None \n","\n","Estad√≠sticas descriptivas de columnas num√©ricas:  \n","          survived      pclass         age       sibsp       parch        fare\n","count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n","mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n","std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n","min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n","25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n","50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n","75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n","max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"]}]},{"cell_type":"markdown","source":["---\n","\n","# **An√°lisis de la Informaci√≥n General del DataFrame**\n","\n","El resultado de la funci√≥n `df.info()` nos proporciona una visi√≥n general crucial de nuestro conjunto de datos, y en este caso, revela aspectos importantes que no hab√≠amos encontrado en nuestros ejemplos anteriores con datos puramente num√©ricos y completos.\n","\n","En primer lugar, vemos que tenemos un DataFrame con **891 entradas (filas)**, indexadas desde 0 hasta 890. Esto representa el n√∫mero total de observaciones en nuestro conjunto de datos.\n","\n","A continuaci√≥n, se detalla la informaci√≥n de cada una de las **15 columnas**. Para cada columna, observamos dos aspectos fundamentales:\n","\n","* **`Non-Null Count` (Conteo de Valores No Nulos):** Esta columna nos indica cu√°ntos valores **no faltantes** hay en cada una de las columnas. Aqu√≠ es donde encontramos una novedad importante: **algunas columnas tienen menos de 891 valores no nulos**. Espec√≠ficamente, notamos que la columna `age` tiene solo 714 valores no nulos, y la columna `embarked` y `embark_town` tienen 889. La columna m√°s llamativa es `deck`, con tan solo 203 valores no nulos. **Esto significa que tenemos datos faltantes en estas columnas.** En un contexto de Machine Learning, los datos faltantes son un desaf√≠o que debemos abordar, ya que muchos algoritmos no pueden trabajar directamente con ellos. Necesitaremos aplicar t√©cnicas de **imputaci√≥n** (reemplazar los valores faltantes con alg√∫n valor estimado) o considerar eliminar las columnas o filas con demasiados faltantes, dependiendo del contexto y del impacto en nuestro modelo.\n","\n","* **`Dtype` (Tipo de Dato):** Esta columna nos muestra el tipo de dato que contiene cada columna. Aqu√≠ tambi√©n encontramos novedades importantes con respecto a nuestros conjuntos de datos anteriores:\n","    * **`object`:** Varias columnas como `sex`, `embarked`, `who`, `embark_town`, y `alive` son de tipo `object`. Esto generalmente indica que contienen **cadenas de texto (strings)**. Los algoritmos de Machine Learning, en su mayor√≠a, trabajan con datos num√©ricos. Por lo tanto, necesitaremos aplicar t√©cnicas de **codificaci√≥n (encoding)** para convertir estas variables categ√≥ricas en representaciones num√©ricas que los modelos puedan entender.\n","    * **`category`:** Las columnas `class` y `deck` son de tipo `category`. Este es un tipo de dato eficiente para variables categ√≥ricas con un n√∫mero limitado de valores distintos. Internamente, Pandas representa estas categor√≠as con n√∫meros, lo que puede ser beneficioso para la memoria y la velocidad. Sin embargo, al igual que con las columnas `object`, es posible que necesitemos considerar c√≥mo se interpretan estas categor√≠as en nuestros modelos de ML.\n","    * **`bool`:** Las columnas `adult_male` y `alone` son de tipo booleano, representando valores True o False. Muchos algoritmos pueden trabajar directamente con datos booleanos, interpret√°ndolos como 0 y 1.\n","    * **`int64` y `float64`:** Estas son columnas num√©ricas, como las que ya hemos trabajado (`survived`, `pclass`, `sibsp`, `parch`, `fare`, `age`). `int64` representa n√∫meros enteros, mientras que `float64` representa n√∫meros de punto flotante.\n","\n","Finalmente, la secci√≥n de **`memory usage`** nos indica la cantidad de memoria que est√° utilizando este DataFrame. Esto puede ser importante cuando trabajamos con conjuntos de datos muy grandes.\n","\n","El √∫ltimo `None` que aparece es el valor de retorno de la funci√≥n `df.info()`, que en Python es `None` ya que su prop√≥sito principal es imprimir la informaci√≥n, no devolver un objeto.\n","\n","**En resumen, al enfrentarnos a este nuevo conjunto de datos, debemos prestar especial atenci√≥n a:**\n","\n","1.  **Los valores faltantes:** Identificar las columnas afectadas y decidir la estrategia para manejarlos.\n","2.  **Los tipos de datos no num√©ricos (`object` y `category`):** Planificar c√≥mo vamos a codificar estas variables para que puedan ser utilizadas por nuestros modelos de Machine Learning.\n","\n","Estos son pasos cruciales en el preprocesamiento de datos para construir modelos efectivos, especialmente cuando trabajamos con datos del mundo real que rara vez son perfectos y puramente num√©ricos.\n","\n","---"],"metadata":{"id":"vBksgUsxoPOD"}},{"cell_type":"markdown","source":["\n","---\n","\n","# **üîπ Paso 2: Un Primer Modelo Simplificado - Eliminaci√≥n de Datos No Num√©ricos y Faltantes**\n","\n","En este segundo paso, vamos a realizar un ciclo completo de Machine Learning: divisi√≥n del dataset, entrenamiento de un modelo, realizaci√≥n de predicciones y evaluaci√≥n de su rendimiento. Sin embargo, para simplificar este primer ejercicio y enfocarnos en el flujo general, vamos a tomar un atajo para lidiar con las columnas que no son num√©ricas y las filas que contienen valores faltantes.\n","\n","**Importante:** Esta es una estrategia simplificada y **no es la mejor pr√°ctica** en la mayor√≠a de los escenarios reales. Eliminar columnas enteras con informaci√≥n potencialmente √∫til y filas con datos faltantes puede llevar a una p√©rdida significativa de informaci√≥n y a un modelo sub√≥ptimo. En pasos posteriores, exploraremos t√©cnicas m√°s sofisticadas para manejar estos problemas.\n","\n","En este paso, nuestro objetivo principal es tener un primer modelo funcional, aunque sea basado en una versi√≥n reducida de nuestros datos. Para ello, realizaremos las siguientes acciones:\n","\n","1.  **Identificar y eliminar las columnas no num√©ricas:** Seleccionaremos solo las columnas con tipos de datos num√©ricos (enteros y flotantes) para poder alimentar directamente nuestro modelo.\n","2.  **Eliminar las filas con valores faltantes (NaN):** Removeremos cualquier fila que contenga al menos un valor faltante en cualquiera de las columnas restantes.\n","\n","Una vez que tengamos un dataset limpio (aunque reducido), procederemos con los pasos habituales:\n","\n","1.  **Dividir el dataset** en un conjunto de entrenamiento y un conjunto de prueba.\n","2.  **Seleccionar y entrenar un modelo de clasificaci√≥n.** Para este primer ejemplo, utilizaremos un modelo simple como la Regresi√≥n Log√≠stica.\n","3.  **Realizar predicciones** sobre el conjunto de prueba.\n","4.  **Evaluar el rendimiento** del modelo utilizando una m√©trica adecuada para la clasificaci√≥n, como la exactitud (accuracy).\n","\n","Este proceso nos dar√° una primera idea del flujo de trabajo completo en un problema de clasificaci√≥n, aunque con las limitaciones impuestas por nuestra estrategia simplificada de manejo de datos.\n","\n","---\n","\n","Aqu√≠ tienes el c√≥digo en Google Colab para llevar a cabo estos pasos:"],"metadata":{"id":"znqeD9QWqQg9"}},{"cell_type":"code","source":["\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Asumiendo que 'df' es tu DataFrame cargado en el Paso 1\n","\n","# 1. Seleccionar solo las columnas num√©ricas\n","df_numeric = df.select_dtypes(include=['int64', 'float64'])\n","print(\"DataFrame solo con columnas num√©ricas:\")\n","print(df_numeric.head())\n","print(\"\\nForma del DataFrame num√©rico:\", df_numeric.shape)\n","\n","# 2. Eliminar las filas con valores faltantes\n","df_cleaned = df_numeric.dropna()\n","print(\"\\nDataFrame despu√©s de eliminar filas con NaN:\")\n","print(df_cleaned.head())\n","print(\"\\nForma del DataFrame limpio:\", df_cleaned.shape)\n","\n","# 3. Definir las variables predictoras (X) y la variable objetivo (y)\n","X = df_cleaned.drop('survived', axis=1)\n","y = df_cleaned['survived']\n","\n","# 4. Dividir el dataset en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nForma de X_train:\", X_train.shape)\n","print(\"Forma de X_test:\", X_test.shape)\n","print(\"Forma de y_train:\", y_train.shape)\n","print(\"Forma de y_test:\", y_test.shape)\n","\n","# 5. Inicializar y entrenar un modelo de Regresi√≥n Log√≠stica\n","model = LogisticRegression(solver='liblinear', random_state=42)\n","model.fit(X_train, y_train)\n","\n","# 6. Realizar predicciones en el conjunto de prueba\n","y_pred = model.predict(X_test)\n","\n","# 7. Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"\\nExactitud del modelo en el conjunto de prueba:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"187mP8mVqWbD","executionInfo":{"status":"ok","timestamp":1744640396265,"user_tz":180,"elapsed":31,"user":{"displayName":"Hector Estigarribia FCyT UNCA","userId":"04796158545147043803"}},"outputId":"3bab5743-c288-40aa-8dbb-d9acfcae8e2e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame solo con columnas num√©ricas:\n","   survived  pclass   age  sibsp  parch     fare\n","0         0       3  22.0      1      0   7.2500\n","1         1       1  38.0      1      0  71.2833\n","2         1       3  26.0      0      0   7.9250\n","3         1       1  35.0      1      0  53.1000\n","4         0       3  35.0      0      0   8.0500\n","\n","Forma del DataFrame num√©rico: (891, 6)\n","\n","DataFrame despu√©s de eliminar filas con NaN:\n","   survived  pclass   age  sibsp  parch     fare\n","0         0       3  22.0      1      0   7.2500\n","1         1       1  38.0      1      0  71.2833\n","2         1       3  26.0      0      0   7.9250\n","3         1       1  35.0      1      0  53.1000\n","4         0       3  35.0      0      0   8.0500\n","\n","Forma del DataFrame limpio: (714, 6)\n","\n","Forma de X_train: (571, 5)\n","Forma de X_test: (143, 5)\n","Forma de y_train: (571,)\n","Forma de y_test: (143,)\n","\n","Exactitud del modelo en el conjunto de prueba: 0.7062937062937062\n"]}]},{"cell_type":"markdown","source":["\n","# Explicaci√≥n del C√≥digo Paso 2:\n","\n","* **`df.select_dtypes(include=['int64', 'float64'])`:** Selecciona del DataFrame original (`df`) solo las columnas cuyos tipos de datos son enteros de 64 bits (`int64`) o n√∫meros de punto flotante de 64 bits (`float64`).\n","* **`df_numeric.dropna()`:** Crea un nuevo DataFrame (`df_cleaned`) eliminando todas las filas que contengan al menos un valor `NaN` (Not a Number) en cualquiera de las columnas.\n","* **Definici√≥n de `X` e `y`:** Asumimos que la columna `'survived'` es nuestra variable objetivo (la que queremos predecir). Separamos las caracter√≠sticas predictoras (`X`) eliminando la columna objetivo y la variable objetivo (`y`). **Si tu objetivo es diferente, aseg√∫rate de ajustar el nombre de la columna.**\n","* **`train_test_split()`:** Divide nuestros datos limpios en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%). `random_state` se utiliza para asegurar la reproducibilidad de la divisi√≥n.\n","* **`LogisticRegression()`:** Inicializamos un modelo de Regresi√≥n Log√≠stica. El argumento `solver='liblinear'` es una buena opci√≥n para datasets peque√±os o medianos.\n","* **`model.fit(X_train, y_train)`:** Entrenamos el modelo utilizando el conjunto de entrenamiento. El modelo aprende la relaci√≥n entre las caracter√≠sticas en `X_train` y la variable objetivo en `y_train`.\n","* **`model.predict(X_test)`:** Utilizamos el modelo entrenado para hacer predicciones sobre el conjunto de prueba (`X_test`).\n","* **`accuracy_score(y_test, y_pred)`:** Comparamos las predicciones del modelo (`y_pred`) con los valores reales de la variable objetivo en el conjunto de prueba (`y_test`) para calcular la exactitud, que es el porcentaje de predicciones correctas."],"metadata":{"id":"Qw8XdDD_qfY5"}},{"cell_type":"markdown","source":["---\n","\n","# **üîπ Paso 3: Imputaci√≥n y Codificaci√≥n para un Mejor Preprocesamiento**\n","*En este Paso, vamos a abordar de manera m√°s adecuada los desaf√≠os de los datos no num√©ricos y los valores faltantes. En lugar de simplemente eliminarlos, aplicaremos t√©cnicas de **imputaci√≥n** para rellenar los valores faltantes y de **codificaci√≥n** para convertir las variables categ√≥ricas en un formato num√©rico que nuestros modelos puedan entender.*\n","\n","En el paso anterior, realizamos un ciclo completo de Machine Learning de forma simplificada, eliminando informaci√≥n valiosa al descartar columnas no num√©ricas y filas con datos faltantes. En este Paso 3, vamos a mejorar significativamente nuestro preprocesamiento de datos aplicando t√©cnicas m√°s sofisticadas:\n","\n","1.  **Imputaci√≥n de Valores Faltantes:** Para las columnas que contienen valores `NaN`, utilizaremos estrategias para estimar y reemplazar estos valores faltantes. La elecci√≥n de la estrategia de imputaci√≥n depender√° del tipo de dato de la columna y de la naturaleza de los datos. Algunas estrategias comunes incluyen:\n","    * Para columnas num√©ricas: Imputar con la media, la mediana o un valor constante.\n","    * Para columnas categ√≥ricas: Imputar con la moda (el valor m√°s frecuente) o una nueva categor√≠a como \"Desconocido\".\n","\n","2.  **Codificaci√≥n de Variables Categ√≥ricas:** Las columnas con tipos de datos `object` o `category` contienen informaci√≥n categ√≥rica que la mayor√≠a de los algoritmos de Machine Learning no pueden procesar directamente. Necesitamos convertirlas a un formato num√©rico. Algunas t√©cnicas comunes de codificaci√≥n son:\n","    * **One-Hot Encoding:** Crea nuevas columnas binarias (0 o 1) para cada categor√≠a √∫nica en la variable original. Es √∫til cuando las categor√≠as no tienen un orden inherente.\n","    * **Label Encoding:** Asigna un n√∫mero entero √∫nico a cada categor√≠a. Es apropiado para variables ordinales (donde las categor√≠as tienen un orden).\n","\n","Nuestro objetivo en este paso es preparar un dataset m√°s completo y representativo para el entrenamiento de modelos, reteniendo la informaci√≥n valiosa que descartamos en el paso anterior. Luego, como hicimos antes, dividiremos el dataset preprocesado, entrenaremos un modelo y lo evaluaremos.\n","\n","---\n","\n","Aqu√≠ tienes el c√≥digo en Google Colab para realizar la imputaci√≥n y la codificaci√≥n. Vamos a aplicar estrategias comunes, pero recuerda que la elecci√≥n de la mejor estrategia a menudo requiere un an√°lisis m√°s profundo de los datos.\n"],"metadata":{"id":"9S5syFtOr61y"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Asumiendo que 'df' es tu DataFrame cargado en el Paso 1\n","\n","# 1. Separar la variable objetivo\n","X = df.drop('survived', axis=1)\n","y = df['survived']\n","\n","# 2. Identificar columnas num√©ricas y categ√≥ricas\n","numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns\n","\n","# 3. Imputar valores faltantes\n","# Imputaci√≥n num√©rica con la media\n","numerical_imputer = SimpleImputer(strategy='mean')\n","X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])\n","\n","# Imputaci√≥n categ√≥rica con la moda\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])\n","\n","print(\"DataFrame despu√©s de la imputaci√≥n:\")\n","print(X.head())\n","print(\"\\nValores nulos despu√©s de la imputaci√≥n:\")\n","print(X.isnull().sum())\n","\n","# 4. Codificar variables categ√≥ricas usando One-Hot Encoding\n","encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse=False para obtener un array NumPy\n","X_encoded = encoder.fit_transform(X[categorical_cols])\n","feature_names = encoder.get_feature_names_out(categorical_cols)\n","X_encoded_df = pd.DataFrame(X_encoded, index=X.index, columns=feature_names)\n","\n","# Concatenar las columnas num√©ricas con las columnas codificadas\n","X_processed = pd.concat([X[numerical_cols], X_encoded_df], axis=1)\n","\n","print(\"\\nDataFrame despu√©s del One-Hot Encoding:\")\n","print(X_processed.head())\n","print(\"\\nForma del DataFrame preprocesado:\", X_processed.shape)\n","\n","# 5. Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nForma de X_train:\", X_train.shape)\n","print(\"Forma de X_test:\", X_test.shape)\n","print(\"Forma de y_train:\", y_train.shape)\n","print(\"Forma de y_test:\", y_test.shape)\n","\n","# 6. Inicializar y entrenar un modelo de Regresi√≥n Log√≠stica\n","model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# 7. Realizar predicciones en el conjunto de prueba\n","y_pred = model.predict(X_test)\n","\n","# 8. Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"\\nExactitud del modelo en el conjunto de prueba (con imputaci√≥n y codificaci√≥n simplificadas):\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQORdBnDsZNJ","executionInfo":{"status":"ok","timestamp":1744641160071,"user_tz":180,"elapsed":260,"user":{"displayName":"Hector Estigarribia FCyT UNCA","userId":"04796158545147043803"}},"outputId":"6b29ea5d-b984-48b0-e440-3b3d09f39553"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame despu√©s de la imputaci√≥n:\n","   pclass     sex   age  sibsp  parch     fare embarked  class    who  \\\n","0     3.0    male  22.0    1.0    0.0   7.2500        S  Third    man   \n","1     1.0  female  38.0    1.0    0.0  71.2833        C  First  woman   \n","2     3.0  female  26.0    0.0    0.0   7.9250        S  Third  woman   \n","3     1.0  female  35.0    1.0    0.0  53.1000        S  First  woman   \n","4     3.0    male  35.0    0.0    0.0   8.0500        S  Third    man   \n","\n","  adult_male deck  embark_town alive  alone  \n","0       True    C  Southampton    no  False  \n","1      False    C    Cherbourg   yes  False  \n","2      False    C  Southampton   yes   True  \n","3      False    C  Southampton   yes  False  \n","4       True    C  Southampton    no   True  \n","\n","Valores nulos despu√©s de la imputaci√≥n:\n","pclass         0\n","sex            0\n","age            0\n","sibsp          0\n","parch          0\n","fare           0\n","embarked       0\n","class          0\n","who            0\n","adult_male     0\n","deck           0\n","embark_town    0\n","alive          0\n","alone          0\n","dtype: int64\n","\n","DataFrame despu√©s del One-Hot Encoding:\n","   pclass   age  sibsp  parch     fare  sex_female  sex_male  embarked_C  \\\n","0     3.0  22.0    1.0    0.0   7.2500         0.0       1.0         0.0   \n","1     1.0  38.0    1.0    0.0  71.2833         1.0       0.0         1.0   \n","2     3.0  26.0    0.0    0.0   7.9250         1.0       0.0         0.0   \n","3     1.0  35.0    1.0    0.0  53.1000         1.0       0.0         0.0   \n","4     3.0  35.0    0.0    0.0   8.0500         0.0       1.0         0.0   \n","\n","   embarked_Q  embarked_S  ...  deck_E  deck_F  deck_G  embark_town_Cherbourg  \\\n","0         0.0         1.0  ...     0.0     0.0     0.0                    0.0   \n","1         0.0         0.0  ...     0.0     0.0     0.0                    1.0   \n","2         0.0         1.0  ...     0.0     0.0     0.0                    0.0   \n","3         0.0         1.0  ...     0.0     0.0     0.0                    0.0   \n","4         0.0         1.0  ...     0.0     0.0     0.0                    0.0   \n","\n","   embark_town_Queenstown  embark_town_Southampton  alive_no  alive_yes  \\\n","0                     0.0                      1.0       1.0        0.0   \n","1                     0.0                      0.0       0.0        1.0   \n","2                     0.0                      1.0       0.0        1.0   \n","3                     0.0                      1.0       0.0        1.0   \n","4                     0.0                      1.0       1.0        0.0   \n","\n","   alone_False  alone_True  \n","0          1.0         0.0  \n","1          1.0         0.0  \n","2          0.0         1.0  \n","3          1.0         0.0  \n","4          0.0         1.0  \n","\n","[5 rows x 32 columns]\n","\n","Forma del DataFrame preprocesado: (891, 32)\n","\n","Forma de X_train: (712, 32)\n","Forma de X_test: (179, 32)\n","Forma de y_train: (712,)\n","Forma de y_test: (179,)\n","\n","Exactitud del modelo en el conjunto de prueba (con imputaci√≥n y codificaci√≥n simplificadas): 1.0\n"]}]},{"cell_type":"markdown","source":["\n","---\n","\n","# Explicaci√≥n del c√≥digo Paso 3\n","\n","Este c√≥digo representa un flujo de trabajo fundamental en Machine Learning, que incluye la preparaci√≥n de los datos (preprocesamiento) y el entrenamiento de un modelo de clasificaci√≥n. Vamos a desglosar cada secci√≥n:\n","\n","**1. Importaci√≥n de Librer√≠as Necesarias:**\n","\n","```python\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","```\n","\n","* **`import pandas as pd`**: Importa la librer√≠a Pandas, que es esencial para trabajar con estructuras de datos tabulares como los DataFrames. Usaremos `pd` como un alias para referirnos a ella de forma m√°s concisa.\n","* **`from sklearn.model_selection import train_test_split`**: Importa la funci√≥n `train_test_split` de la librer√≠a Scikit-learn (`sklearn`). Esta funci√≥n nos permitir√° dividir nuestro conjunto de datos en dos partes: un conjunto para entrenar nuestro modelo y otro para evaluar su rendimiento.\n","* **`from sklearn.linear_model import LogisticRegression`**: Importa el modelo de clasificaci√≥n de Regresi√≥n Log√≠stica de Scikit-learn. Este es un algoritmo que se utiliza para problemas de clasificaci√≥n binaria (en nuestro caso, predecir si un pasajero sobrevivi√≥ o no).\n","* **`from sklearn.metrics import accuracy_score`**: Importa la funci√≥n `accuracy_score` de Scikit-learn. La exactitud es una m√©trica que nos dice qu√© porcentaje de las predicciones de nuestro modelo fueron correctas.\n","* **`from sklearn.impute import SimpleImputer`**: Importa la clase `SimpleImputer` de Scikit-learn. Esta herramienta nos ayudar√° a manejar los valores faltantes (NaN) en nuestros datos, reemplaz√°ndolos con una estrategia espec√≠fica (como la media o la moda).\n","* **`from sklearn.preprocessing import OneHotEncoder`**: Importa la clase `OneHotEncoder` de Scikit-learn. Esta t√©cnica se utiliza para convertir variables categ√≥ricas (textuales o con un n√∫mero limitado de categor√≠as) en un formato num√©rico que los algoritmos de Machine Learning pueden entender.\n","\n","**2. Separaci√≥n de la Variable Objetivo y las Caracter√≠sticas:**\n","\n","```python\n","# Asumiendo que 'df' es tu DataFrame cargado en el Paso 1\n","\n","# 1. Separar la variable objetivo\n","X = df.drop('survived', axis=1)\n","y = df['survived']\n","```\n","\n","* Asumimos que `df` es el DataFrame que cargamos y analizamos en el Paso 1.\n","* **`X = df.drop('survived', axis=1)`**: Creamos un nuevo DataFrame llamado `X` que contiene todas las columnas del DataFrame original `df`, **excepto** la columna `'survived'`. Consideramos las columnas en `X` como las **caracter√≠sticas predictoras** o variables independientes que usaremos para predecir la variable objetivo. El argumento `axis=1` indica que queremos eliminar una columna.\n","* **`y = df['survived']`**: Creamos una Serie de Pandas llamada `y` que contiene **solo** la columna `'survived'` del DataFrame original. Esta es nuestra **variable objetivo** o variable dependiente, la cual queremos predecir con nuestro modelo.\n","\n","**3. Identificaci√≥n de Tipos de Columnas:**\n","\n","```python\n","# 2. Identificar columnas num√©ricas y categ√≥ricas\n","numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns\n","```\n","\n","* Para aplicar diferentes estrategias de preprocesamiento a diferentes tipos de datos, primero identificamos las columnas num√©ricas y categ√≥ricas en nuestro conjunto de caracter√≠sticas `X`.\n","* **`numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns`**: Selecciona todas las columnas en `X` cuyo tipo de dato es entero (`int64`) o n√∫mero de punto flotante (`float64`) y guarda sus nombres en la lista `numerical_cols`.\n","* **`categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns`**: Selecciona todas las columnas en `X` cuyo tipo de dato es objeto (generalmente texto), categor√≠a o booleano, y guarda sus nombres en la lista `categorical_cols`.\n","\n","**4. Imputaci√≥n de Valores Faltantes:**\n","\n","```python\n","# 3. Imputar valores faltantes\n","# Imputaci√≥n num√©rica con la media\n","numerical_imputer = SimpleImputer(strategy='mean')\n","X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])\n","\n","# Imputaci√≥n categ√≥rica con la moda\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])\n","\n","print(\"DataFrame despu√©s de la imputaci√≥n:\")\n","print(X.head())\n","print(\"\\nValores nulos despu√©s de la imputaci√≥n:\")\n","print(X.isnull().sum())\n","```\n","\n","* Aqu√≠ manejamos los valores faltantes (NaN) que pudimos haber identificado en el Paso 1.\n","* **`numerical_imputer = SimpleImputer(strategy='mean')`**: Creamos un objeto `SimpleImputer` para las columnas num√©ricas. La estrategia `'mean'` indica que los valores faltantes en estas columnas ser√°n reemplazados por la media de los valores existentes en cada columna.\n","* **`X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])`**: Primero, `fit_transform` calcula la media de cada columna num√©rica en `X` y luego reemplaza los valores faltantes en esas mismas columnas con sus respectivas medias. El resultado se asigna de nuevo a las columnas num√©ricas de `X`, modificando el DataFrame.\n","* **`categorical_imputer = SimpleImputer(strategy='most_frequent')`**: Creamos otro `SimpleImputer` para las columnas categ√≥ricas. La estrategia `'most_frequent'` indica que los valores faltantes en estas columnas ser√°n reemplazados por la moda (el valor que aparece con mayor frecuencia) en cada columna.\n","* **`X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])`**: De manera similar, `fit_transform` calcula la moda de cada columna categ√≥rica en `X` y luego reemplaza los valores faltantes con sus respectivas modas. El resultado se asigna de vuelta a las columnas categ√≥ricas de `X`.\n","* Las siguientes l√≠neas de `print` muestran las primeras filas del DataFrame despu√©s de la imputaci√≥n y la cantidad de valores nulos por columna, lo que deber√≠a confirmar que ya no tenemos valores faltantes.\n","\n","**5. Codificaci√≥n de Variables Categ√≥ricas (One-Hot Encoding):**\n","\n","```python\n","# 4. Codificar variables categ√≥ricas usando One-Hot Encoding\n","encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse=False para obtener un array NumPy\n","X_encoded = encoder.fit_transform(X[categorical_cols])\n","feature_names = encoder.get_feature_names_out(categorical_cols)\n","X_encoded_df = pd.DataFrame(X_encoded, index=X.index, columns=feature_names)\n","\n","# Concatenar las columnas num√©ricas con las columnas codificadas\n","X_processed = pd.concat([X[numerical_cols], X_encoded_df], axis=1)\n","\n","print(\"\\nDataFrame despu√©s del One-Hot Encoding:\")\n","print(X_processed.head())\n","print(\"\\nForma del DataFrame preprocesado:\", X_processed.shape)\n","```\n","\n","* Aqu√≠ convertimos las variables categ√≥ricas en un formato num√©rico adecuado para nuestro modelo de Regresi√≥n Log√≠stica.\n","* **`encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)`**: Creamos un objeto `OneHotEncoder`.\n","    * `handle_unknown='ignore'` le dice al encoder que ignore las categor√≠as desconocidas que puedan aparecer en el conjunto de prueba pero no en el de entrenamiento, en lugar de generar un error.\n","    * `sparse_output=False` hace que la salida sea un array NumPy denso en lugar de una matriz dispersa, lo cual es m√°s f√°cil de trabajar en este caso.\n","* **`X_encoded = encoder.fit_transform(X[categorical_cols])`**: Primero, `fit` aprende las categor√≠as √∫nicas presentes en cada columna categ√≥rica de `X`. Luego, `transform` aplica la codificaci√≥n one-hot, creando nuevas columnas binarias (0 o 1) para cada categor√≠a √∫nica. El resultado es un array NumPy llamado `X_encoded`.\n","* **`feature_names = encoder.get_feature_names_out(categorical_cols)`**: Obtiene los nombres de las nuevas columnas creadas por el one-hot encoding. Estos nombres se basan en los nombres de las columnas originales y las categor√≠as √∫nicas dentro de ellas (por ejemplo, `sex_male`, `embarked_S`).\n","* **`X_encoded_df = pd.DataFrame(X_encoded, index=X.index, columns=feature_names)`**: Convertimos el array NumPy `X_encoded` en un DataFrame de Pandas llamado `X_encoded_df`, utilizando el mismo √≠ndice que el DataFrame original `X` y los nombres de las caracter√≠sticas obtenidos del encoder.\n","* **`X_processed = pd.concat([X[numerical_cols], X_encoded_df], axis=1)`**: Concatenamos (unimos) las columnas num√©ricas originales de `X` con las nuevas columnas codificadas en `X_encoded_df` a lo largo de las columnas (`axis=1`). El resultado es nuestro DataFrame preprocesado `X_processed`, que ahora contiene solo datos num√©ricos.\n","* Las siguientes l√≠neas de `print` muestran las primeras filas y la forma del DataFrame preprocesado. El n√∫mero de columnas habr√° aumentado debido a la expansi√≥n de las variables categ√≥ricas.\n","\n","**6. Divisi√≥n de los Datos en Conjuntos de Entrenamiento y Prueba:**\n","\n","```python\n","# 5. Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nForma de X_train:\", X_train.shape)\n","print(\"Forma de X_test:\", X_test.shape)\n","print(\"Forma de y_train:\", y_train.shape)\n","print(\"Forma de y_test:\", y_test.shape)\n","```\n","\n","* Dividimos nuestro conjunto de datos preprocesado en dos partes: un conjunto de entrenamiento para que el modelo aprenda y un conjunto de prueba para evaluar qu√© tan bien generaliza a datos nuevos.\n","* **`X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)`**: La funci√≥n `train_test_split` toma las caracter√≠sticas preprocesadas (`X_processed`) y la variable objetivo (`y`) como entrada.\n","    * `test_size=0.2` especifica que el 20% de los datos se utilizar√° para el conjunto de prueba, mientras que el 80% restante se utilizar√° para el conjunto de entrenamiento.\n","    * `random_state=42` es una semilla para el generador de n√∫meros aleatorios. Usar una semilla asegura que la divisi√≥n de los datos sea la misma cada vez que se ejecuta el c√≥digo, lo que facilita la reproducibilidad de los resultados.\n","* Las l√≠neas de `print` muestran la forma (n√∫mero de filas y columnas) de los conjuntos de entrenamiento y prueba para las caracter√≠sticas (`X_train`, `X_test`) y la variable objetivo (`y_train`, `y_test`).\n","\n","**7. Inicializaci√≥n y Entrenamiento del Modelo de Regresi√≥n Log√≠stica:**\n","\n","```python\n","# 6. Inicializar y entrenar un modelo de Regresi√≥n Log√≠stica\n","model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n","model.fit(X_train, y_train)\n","```\n","\n","* Creamos una instancia del modelo de Regresi√≥n Log√≠stica y lo entrenamos con nuestros datos de entrenamiento preprocesados.\n","* **`model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)`**: Inicializamos un objeto `LogisticRegression`.\n","    * `solver='liblinear'` es un algoritmo de optimizaci√≥n adecuado para conjuntos de datos peque√±os y medianos como el nuestro.\n","    * `random_state=42` asegura la reproducibilidad del entrenamiento del modelo.\n","    * `max_iter=1000` establece el n√∫mero m√°ximo de iteraciones para que el solver converja.\n","* **`model.fit(X_train, y_train)`**: Entrenamos el modelo utilizando el conjunto de entrenamiento (`X_train` para las caracter√≠sticas y `y_train` para la variable objetivo). Durante el entrenamiento, el modelo aprende la relaci√≥n entre las caracter√≠sticas y la probabilidad de supervivencia.\n","\n","**8. Realizaci√≥n de Predicciones en el Conjunto de Prueba:**\n","\n","```python\n","# 7. Realizar predicciones en el conjunto de prueba\n","y_pred = model.predict(X_test)\n","```\n","\n","* Una vez que el modelo est√° entrenado, lo utilizamos para hacer predicciones sobre el conjunto de prueba, que el modelo nunca ha visto antes.\n","* **`y_pred = model.predict(X_test)`**: El m√©todo `predict` del modelo toma las caracter√≠sticas del conjunto de prueba (`X_test`) como entrada y devuelve un array (`y_pred`) que contiene las predicciones de supervivencia (0 o 1) para cada pasajero en el conjunto de prueba.\n","\n","**9. Evaluaci√≥n del Rendimiento del Modelo:**\n","\n","```python\n","# 8. Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"\\nExactitud del modelo en el conjunto de prueba (con imputaci√≥n y codificaci√≥n simplificadas):\", accuracy)\n","```\n","\n","* Finalmente, evaluamos qu√© tan bien se desempe√±√≥ nuestro modelo comparando sus predicciones con los valores reales de la variable objetivo en el conjunto de prueba.\n","* **`accuracy = accuracy_score(y_test, y_pred)`**: La funci√≥n `accuracy_score` compara las etiquetas verdaderas del conjunto de prueba (`y_test`) con las predicciones de nuestro modelo (`y_pred`) y calcula la exactitud, que es la proporci√≥n de predicciones correctas.\n","* La l√≠nea de `print` muestra la exactitud del modelo en el conjunto de prueba, d√°ndonos una idea de su rendimiento en datos no vistos.\n","\n","---\n","\n","Esta explicaci√≥n detallada deber√≠a proporcionar a tus estudiantes una comprensi√≥n clara de cada paso del c√≥digo, desde la importaci√≥n de librer√≠as hasta la evaluaci√≥n del modelo. ¬°Espero que sea √∫til! ¬øQu√© te gustar√≠a abordar a continuaci√≥n?"],"metadata":{"id":"0k-5pt50sfXn"}},{"cell_type":"markdown","source":["\n","---\n","\n","# **Explicaci√≥n de los Resultados Paso 3:**\n","\n","**1. DataFrame despu√©s de la Imputaci√≥n:**\n","\n","```\n","   pclass     sex   age  sibsp  parch     fare embarked  class    who  \\\n","0     3.0    male  22.0    1.0    0.0   7.2500        S  Third    man\n","1     1.0  female  38.0    1.0    0.0  71.2833        C  First  woman\n","2     3.0  female  26.0    0.0    0.0   7.9250        S  Third  woman\n","3     1.0  female  35.0    1.0    0.0  53.1000        S  First  woman\n","4     3.0    male  35.0    0.0    0.0   8.0500        S  Third    man\n","\n","  adult_male deck  embark_town alive  alone\n","0       True    C  Southampton    no  False\n","1      False    C    Cherbourg   yes  False\n","2      False    C  Southampton   yes   True\n","3      False    C  Southampton   yes  False\n","4       True    C  Southampton    no   True\n","```\n","\n","* Este es un vistazo a las primeras filas del DataFrame despu√©s de que aplicamos la **imputaci√≥n de valores faltantes**.\n","* **Columnas Num√©ricas (como `age`, `fare`):** Los valores `NaN` que exist√≠an previamente en estas columnas han sido reemplazados por la **media** de los valores presentes en cada una de esas columnas. Aunque no vemos un ejemplo directo en estas primeras filas (ya que no ten√≠an `NaN`), si hubiera habido un `NaN` en la columna `age`, por ejemplo, se habr√≠a llenado con la edad promedio de todos los pasajeros.\n","* **Columnas Categ√≥ricas (como `embarked`, `deck`, `embark_town`):** Los valores faltantes en estas columnas han sido reemplazados por la **moda**, es decir, el valor m√°s frecuente que aparec√≠a en cada una de estas columnas. En la columna `deck`, vemos una 'C' en todas las primeras filas, lo que podr√≠a indicar que 'C' era la moda (el valor m√°s com√∫n) si algunas de estas filas originalmente ten√≠an un valor faltante en esa columna.\n","\n","**2. Valores nulos despu√©s de la imputaci√≥n:**\n","\n","```\n","pclass         0\n","sex            0\n","age            0\n","sibsp          0\n","parch          0\n","fare           0\n","embarked       0\n","class          0\n","who            0\n","adult_male     0\n","deck           0\n","embark_town    0\n","alive          0\n","alone          0\n","dtype: int64\n","```\n","\n","* Este resultado es crucial. Muestra el **conteo de valores nulos (NaN)** para cada columna despu√©s de aplicar la imputaci√≥n.\n","* Vemos que para cada una de las 14 columnas, el conteo de valores nulos es **0**. Esto significa que nuestra imputaci√≥n ha sido exitosa y **todos los valores faltantes han sido reemplazados** utilizando las estrategias que definimos (media para num√©ricas, moda para categ√≥ricas).\n","\n","**3. DataFrame despu√©s del One-Hot Encoding:**\n","\n","```\n","   pclass   age  sibsp  parch     fare  sex_female  sex_male  embarked_C  \\\n","0     3.0  22.0    1.0    0.0   7.2500         0.0       1.0         0.0\n","1     1.0  38.0    1.0    0.0  71.2833         1.0       0.0         1.0\n","2     3.0  26.0    0.0    0.0   7.9250         1.0       0.0         0.0\n","3     1.0  35.0    1.0    0.0  53.1000         1.0       0.0         0.0\n","4     3.0  35.0    0.0    0.0   8.0500         0.0       1.0         0.0\n","\n","   embarked_Q  embarked_S  ...  deck_E  deck_F  deck_G  embark_town_Cherbourg  \\\n","0         0.0         1.0  ...     0.0     0.0     0.0                    0.0\n","1         0.0         0.0  ...     0.0     0.0     0.0                    1.0\n","2         0.0         1.0  ...     0.0     0.0     0.0                    0.0\n","3         0.0         1.0  ...     0.0     0.0     0.0                    0.0\n","4         0.0         1.0  ...     0.0     0.0     0.0                    0.0\n","\n","   embark_town_Queenstown  embark_town_Southampton  alive_no  alive_yes  \\\n","0                     0.0                      1.0       1.0        0.0\n","1                     0.0                      0.0       0.0        1.0\n","2                     0.0                      1.0       0.0        1.0\n","3                     0.0                      1.0       0.0        1.0\n","4                     0.0                      1.0       1.0        0.0\n","\n","   alone_False  alone_True\n","0          1.0         0.0\n","1          1.0         0.0\n","2          0.0         1.0\n","3          1.0         0.0\n","4          0.0         1.0\n","\n","[5 rows x 32 columns]\n","```\n","\n","* Aqu√≠ vemos el resultado de aplicar **One-Hot Encoding** a las columnas categ√≥ricas.\n","* Las columnas num√©ricas originales (`pclass`, `age`, `sibsp`, `parch`, `fare`) se mantienen intactas.\n","* Las columnas categ√≥ricas han sido transformadas. Por ejemplo, la columna original `sex` con valores 'male' y 'female' se ha convertido en dos nuevas columnas binarias: `sex_female` y `sex_male`. Para cada fila, una de estas columnas tendr√° un valor de 1.0 (indicando la presencia de esa categor√≠a) y la otra tendr√° 0.0.\n","* De manera similar, la columna `embarked` (con valores como 'S', 'C', 'Q') se ha expandido en las columnas `embarked_C`, `embarked_Q`, y `embarked_S`.\n","* Este proceso se ha repetido para todas las columnas categ√≥ricas (`class`, `who`, `adult_male`, `deck`, `embark_town`, `alive`, `alone`), creando nuevas columnas binarias para cada categor√≠a √∫nica dentro de ellas.\n","* La **forma del DataFrame preprocesado** ahora es `(891, 32)`. Originalmente ten√≠amos 15 columnas (incluyendo la variable objetivo `survived`). Despu√©s de eliminar `survived` para el preprocesamiento y aplicar one-hot encoding a las columnas categ√≥ricas, el n√∫mero de columnas ha aumentado significativamente (de 14 a 32) debido a la expansi√≥n de las variables categ√≥ricas en m√∫ltiples columnas binarias.\n","\n","**4. Formas de los conjuntos de entrenamiento y prueba:**\n","\n","```\n","Forma de X_train: (712, 32)\n","Forma de X_test: (179, 32)\n","Forma de y_train: (712,)\n","Forma de y_test: (179,)\n","```\n","\n","* Despu√©s de preprocesar los datos, los dividimos en un **conjunto de entrenamiento** y un **conjunto de prueba**.\n","* `X_train` contiene las caracter√≠sticas preprocesadas para 712 pasajeros que se utilizar√°n para entrenar nuestro modelo. Tiene 32 columnas, correspondientes a las caracter√≠sticas num√©ricas originales y las nuevas columnas creadas por el one-hot encoding.\n","* `X_test` contiene las mismas 32 caracter√≠sticas preprocesadas para 179 pasajeros que se utilizar√°n para evaluar el rendimiento de nuestro modelo entrenado.\n","* `y_train` contiene la variable objetivo (`survived`) para los 712 pasajeros del conjunto de entrenamiento. Tiene una sola dimensi√≥n.\n","* `y_test` contiene la variable objetivo para los 179 pasajeros del conjunto de prueba, que usaremos para comparar con las predicciones de nuestro modelo.\n","\n","**5. Exactitud del modelo en el conjunto de prueba:**\n","\n","```\n","Exactitud del modelo en el conjunto de prueba (con imputaci√≥n y codificaci√≥n simplificadas): 1.0\n","```\n","\n","* Este es el resultado de la **evaluaci√≥n de nuestro modelo de Regresi√≥n Log√≠stica** en el conjunto de prueba.\n","* Una exactitud de **1.0** (o 100%) significa que **nuestro modelo predijo correctamente la supervivencia de todos los pasajeros en el conjunto de prueba**.\n","\n","**Puntos Importantes :**\n","\n","* Hemos logrado **manejar los valores faltantes** utilizando estrategias b√°sicas de imputaci√≥n.\n","* Hemos convertido las **variables categ√≥ricas en un formato num√©rico** que el modelo de Machine Learning puede entender a trav√©s del One-Hot Encoding.\n","* El n√∫mero de caracter√≠sticas (columnas) ha aumentado debido al One-Hot Encoding.\n","* Hemos dividido los datos preprocesados para entrenar y evaluar el modelo.\n","* El modelo de Regresi√≥n Log√≠stica, con este preprocesamiento, ha alcanzado una exactitud perfecta en el conjunto de prueba.\n","\n","**Precauci√≥n sobre la Exactitud del 100%:**\n","\n","Es importante mencionar que una exactitud del 100% en un conjunto de prueba **debe ser examinada con escepticismo**, especialmente en problemas del mundo real. Podr√≠a indicar:\n","\n","* **Un conjunto de prueba demasiado peque√±o:** Con pocos ejemplos, es m√°s f√°cil obtener una predicci√≥n perfecta por casualidad.\n","* **Fugas de informaci√≥n (data leakage):** Podr√≠a haber alguna informaci√≥n en las caracter√≠sticas que indirectamente revela la variable objetivo.\n","* **Un problema demasiado simple para el modelo:** En algunos casos raros, el problema podr√≠a ser inherentemente muy f√°cil de predecir.\n","\n","En los siguientes pasos, podr√≠amos explorar estas posibilidades y aprender sobre la importancia de validar los modelos de manera robusta.\n"],"metadata":{"id":"hxfiKZo-vXNl"}},{"cell_type":"markdown","source":["\n","# **¬øPor Qu√© Usamos One-Hot Encoding en Lugar de Label Encoding para Variables Categ√≥ricas?**\n","\n","La elecci√≥n entre One-Hot Encoding y Label Encoding depende fundamentalmente de la **naturaleza de la variable categ√≥rica** que estamos codificando, espec√≠ficamente si existe o no una **relaci√≥n de orden inherente** entre sus categor√≠as.\n","\n","**Label Encoding (Codificaci√≥n de Etiquetas):**\n","\n","* **¬øQu√© hace?** Asigna un n√∫mero entero √∫nico a cada categor√≠a en la variable. Por ejemplo, si la columna 'Color' tiene las categor√≠as 'Rojo', 'Verde' y 'Azul', Label Encoding podr√≠a convertirlas en 0, 1 y 2 respectivamente.\n","* **¬øCu√°ndo podr√≠a ser apropiado?** Label Encoding es m√°s apropiado para **variables categ√≥ricas ordinales**. Una variable es ordinal si sus categor√≠as tienen un orden o una jerarqu√≠a significativa. Ejemplos:\n","    * 'Tama√±o': 'Peque√±o', 'Mediano', 'Grande' (existe un orden)\n","    * 'Nivel de Educaci√≥n': 'Primaria', 'Secundaria', 'Universidad' (existe un orden)\n","    * 'Satisfacci√≥n': 'Bajo', 'Medio', 'Alto' (existe un orden)\n","* **¬øCu√°l es el problema con variables nominales?** El principal problema de aplicar Label Encoding a **variables categ√≥ricas nominales** (donde no existe un orden inherente entre las categor√≠as, como 'Color', 'Ciudad', 'Sexo', 'Puerto de Embarque') es que **introduce una noci√≥n de orden o jerarqu√≠a que no existe en realidad**.\n","\n","    * Para nuestro ejemplo de 'Color' (Rojo: 0, Verde: 1, Azul: 2), el modelo podr√≠a interpretar err√≥neamente que 'Verde' es \"mayor que\" 'Rojo' y que 'Azul' es \"mayor que\" ambos. Esto puede confundir al algoritmo y llevar a un rendimiento sub√≥ptimo, ya que el modelo podr√≠a asignar m√°s peso o importancia a ciertas categor√≠as bas√°ndose en el valor num√©rico asignado, en lugar de en su verdadera relaci√≥n con la variable objetivo.\n","\n","**One-Hot Encoding (Codificaci√≥n \"Uno Caliente\"):**\n","\n","* **¬øQu√© hace?** Crea nuevas columnas binarias (con valores 0 o 1) para cada categor√≠a √∫nica en la variable original. Por ejemplo, la columna 'Color' ('Rojo', 'Verde', 'Azul') se convertir√≠a en tres nuevas columnas: 'Color_Rojo', 'Color_Verde' y 'Color_Azul'. Para cada fila, solo una de estas columnas tendr√° un valor de 1 (indicando que esa era la categor√≠a original), mientras que las otras tendr√°n 0.\n","* **¬øCu√°ndo es apropiado?** One-Hot Encoding es la t√©cnica preferida para **variables categ√≥ricas nominales**. Al crear columnas separadas para cada categor√≠a, evitamos introducir cualquier tipo de relaci√≥n de orden o magnitud entre ellas. Cada categor√≠a se trata como independiente de las dem√°s.\n","* **¬øPor qu√© es mejor para variables nominales?**\n","    * **Evita la falsa relaci√≥n de orden:** Cada categor√≠a tiene su propia columna, por lo que el modelo no asume ninguna jerarqu√≠a entre ellas.\n","    * **Permite al modelo aprender relaciones espec√≠ficas:** El modelo puede aprender una relaci√≥n diferente entre cada categor√≠a y la variable objetivo. Por ejemplo, la probabilidad de supervivencia podr√≠a ser diferente para hombres y mujeres, y el One-Hot Encoding permite al modelo capturar estas diferencias individualmente a trav√©s de las columnas 'sex_male' y 'sex_female'.\n","\n","**En el Contexto del Dataset del Titanic:**\n","\n","La mayor√≠a de las variables categ√≥ricas en el dataset del Titanic son **nominales**. Considera ejemplos como:\n","\n","* **`sex` (male/female):** No hay un orden inherente entre ser hombre o mujer en t√©rminos de supervivencia.\n","* **`embarked` (C, Q, S):** Los diferentes puertos de embarque no tienen una relaci√≥n ordinal natural.\n","* **`class` (First, Second, Third):** Aunque existe un orden socioecon√≥mico, al tratarlos directamente como categor√≠as nominales a trav√©s de One-Hot Encoding, permitimos que el modelo aprenda la relaci√≥n espec√≠fica de cada clase con la supervivencia sin imponer un gradiente lineal. (Podr√≠a haber argumentos para Label Encoding aqu√≠ si se quisiera enfatizar el orden, pero One-Hot es m√°s com√∫n para evitar suposiciones).\n","* **Otras como `who`, `deck`, `embark_town`, `alive`, `alone` tambi√©n son principalmente nominales.**\n","\n","Al aplicar Label Encoding a estas variables nominales, estar√≠amos impl√≠citamente diciendo al modelo que existe una \"mayor que\" o \"menor que\" entre las categor√≠as, lo cual no es cierto y podr√≠a distorsionar el aprendizaje del modelo.\n","\n","**Excepci√≥n Potencial:**\n","\n","Podr√≠a haber algunas variables categ√≥ricas en el dataset (como `class` mencionado anteriormente o quiz√°s alguna codificaci√≥n de rangos de edad si se hubiera hecho como categor√≠as) donde un Label Encoding *podr√≠a* ser considerado si el orden es realmente significativo y se quiere que el modelo lo interprete como tal. Sin embargo, incluso en esos casos, One-Hot Encoding suele ser m√°s seguro para evitar interpretaciones err√≥neas por parte del modelo.\n","\n","**En resumen, usamos One-Hot Encoding para las variables categ√≥ricas nominales en el dataset del Titanic (y en muchos otros problemas de clasificaci√≥n) porque:**\n","\n","* **Preserva la independencia de las categor√≠as.**\n","* **Evita introducir relaciones de orden o magnitud artificiales.**\n","* **Permite que el modelo aprenda la influencia espec√≠fica de cada categor√≠a en la variable objetivo.**\n","\n","Adem√°s de los conceptos, siempre se debe enfatizar la importancia de **entender la naturaleza de los datos** antes de aplicar cualquier t√©cnica de codificaci√≥n."],"metadata":{"id":"3kkODJH402__"}}]}